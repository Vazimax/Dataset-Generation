#!/usr/bin/env python3
"""
Comprehensive Vulnerability Detection Testing

This script tests our critical CVE dataset against a comprehensive set of
vulnerability detection tools, including both traditional static analysis
and ML-enhanced tools that are readily available.

"""

import json
import os
import subprocess
import tempfile
import time
import re
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime
import logging

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('comprehensive_vulnerability_detection_testing.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class DetectionResult:
    """Result of vulnerability detection"""
    cve_id: str
    tool_name: str
    detected: bool
    confidence: float
    detection_time: float
    tool_output: str
    error_message: Optional[str] = None

@dataclass
class ComprehensiveBenchmark:
    """Overall comprehensive benchmark result"""
    dataset_name: str
    total_samples: int
    detection_results: List[DetectionResult]
    overall_metrics: Dict
    tool_performance: Dict[str, Dict]
    timestamp: str

class ComprehensiveVulnerabilityTester:
    """Test vulnerability detection using comprehensive set of tools"""
    
    def __init__(self):
        self.tools = self._initialize_comprehensive_tools()
        self.results = []
    
    def _initialize_comprehensive_tools(self) -> Dict:
        """Initialize comprehensive set of vulnerability detection tools"""
        
        tools = {
            # Traditional Static Analysis Tools
            'cppcheck': {
                'name': 'Cppcheck',
                'type': 'static_analysis',
                'command': 'cppcheck',
                'args': ['--enable=all', '--xml', '--xml-version=2'],
                'description': 'Traditional static analysis tool for C/C++',
                'ml_capabilities': 'Pattern-based detection'
            },
            'clang': {
                'name': 'Clang Static Analyzer',
                'type': 'static_analysis',
                'command': 'clang',
                'args': ['--analyze', '-Xanalyzer', '-analyzer-output=plist'],
                'description': 'Clang static analyzer for C/C++',
                'ml_capabilities': 'Semantic analysis'
            },
            'gcc': {
                'name': 'GCC Security Warnings',
                'type': 'compiler_warnings',
                'command': 'gcc',
                'args': ['-Wall', '-Wextra', '-Wformat-security', '-Wformat=2'],
                'description': 'GCC compiler with security warnings',
                'ml_capabilities': 'Pattern-based warnings'
            },
            'flawfinder': {
                'name': 'Flawfinder',
                'type': 'static_analysis',
                'command': 'flawfinder',
                'args': ['--html', '--context'],
                'description': 'Static analysis tool for C/C++',
                'ml_capabilities': 'Pattern matching'
            },
            
            # ML-Enhanced Tools
            'bandit': {
                'name': 'Bandit',
                'type': 'ml_enhanced_static_analysis',
                'command': 'bandit',
                'args': ['-r', '-f', 'json'],
                'description': 'ML-enhanced static analysis tool',
                'ml_capabilities': 'Pattern learning and classification'
            },
            'semgrep': {
                'name': 'Semgrep',
                'type': 'ml_enhanced_pattern_matching',
                'command': 'semgrep',
                'args': ['--config=auto', '--json'],
                'description': 'ML-enhanced pattern matching with semantic analysis',
                'ml_capabilities': 'Semantic pattern recognition'
            },
            
            # Advanced Analysis Tools
            'infer': {
                'name': 'Infer',
                'type': 'advanced_static_analysis',
                'command': 'infer',
                'args': ['--', 'gcc', '-c'],
                'description': 'Advanced static analysis with ML components',
                'ml_capabilities': 'Machine learning-based analysis'
            },
            'spotbugs': {
                'name': 'SpotBugs',
                'type': 'static_analysis',
                'command': 'spotbugs',
                'args': ['-textui', '-xml'],
                'description': 'Static analysis tool with ML enhancements',
                'ml_capabilities': 'Pattern learning'
            }
        }
        
        return tools
    
    def install_tools(self) -> Dict[str, bool]:
        """Install available vulnerability detection tools"""
        
        installation_results = {}
        
        # Install Python-based tools
        python_tools = ['bandit', 'semgrep']
        
        for tool in python_tools:
            logger.info(f"üì¶ Installing {tool}...")
            try:
                result = subprocess.run(['pip', 'install', tool], 
                                      capture_output=True, text=True, timeout=300)
                installation_results[tool] = result.returncode == 0
                if result.returncode == 0:
                    logger.info(f"‚úÖ {tool} installed successfully")
                else:
                    logger.warning(f"‚ö†Ô∏è {tool} installation failed: {result.stderr}")
            except Exception as e:
                logger.error(f"‚ùå {tool} installation error: {str(e)}")
                installation_results[tool] = False
        
        # Install system tools (if available)
        system_tools = ['cppcheck', 'flawfinder']
        
        for tool in system_tools:
            logger.info(f"üì¶ Installing {tool} via system package manager...")
            try:
                # Try different package managers
                for package_manager in ['brew', 'apt-get', 'yum', 'pacman']:
                    if package_manager == 'brew':
                        result = subprocess.run(['brew', 'install', tool], 
                                              capture_output=True, text=True, timeout=300)
                    elif package_manager == 'apt-get':
                        result = subprocess.run(['sudo', 'apt-get', 'install', '-y', tool], 
                                              capture_output=True, text=True, timeout=300)
                    elif package_manager == 'yum':
                        result = subprocess.run(['sudo', 'yum', 'install', '-y', tool], 
                                              capture_output=True, text=True, timeout=300)
                    elif package_manager == 'pacman':
                        result = subprocess.run(['sudo', 'pacman', '-S', '--noconfirm', tool], 
                                              capture_output=True, text=True, timeout=300)
                    
                    if result.returncode == 0:
                        installation_results[tool] = True
                        logger.info(f"‚úÖ {tool} installed successfully via {package_manager}")
                        break
                else:
                    installation_results[tool] = False
                    logger.warning(f"‚ö†Ô∏è {tool} installation failed with all package managers")
                    
            except Exception as e:
                logger.error(f"‚ùå {tool} installation error: {str(e)}")
                installation_results[tool] = False
        
        return installation_results
    
    def check_tool_availability(self) -> Dict[str, bool]:
        """Check which tools are available"""
        
        availability = {}
        
        for tool_key, tool_config in self.tools.items():
            try:
                result = subprocess.run([tool_config['command'], '--version'], 
                                      capture_output=True, text=True, timeout=10)
                availability[tool_key] = result.returncode == 0
            except (FileNotFoundError, subprocess.TimeoutExpired):
                availability[tool_key] = False
        
        return availability
    
    def test_dataset(self, dataset_path: str, dataset_name: str, 
                    sample_limit: Optional[int] = None) -> ComprehensiveBenchmark:
        """Test dataset against comprehensive set of tools"""
        
        logger.info(f"üß™ Starting comprehensive vulnerability testing for {dataset_name}")
        
        # Load dataset
        try:
            with open(dataset_path, 'r') as f:
                dataset = json.load(f)
        except FileNotFoundError:
            logger.error(f"Dataset not found: {dataset_path}")
            return None
        
        # Extract samples
        if 'samples' in dataset:
            samples = dataset['samples']
        else:
            samples = dataset
        
        # Limit samples if specified
        if sample_limit and len(samples) > sample_limit:
            samples = samples[:sample_limit]
        
        logger.info(f"Loaded {len(samples)} samples from {dataset_name}")
        
        # Check tool availability
        availability = self.check_tool_availability()
        available_tools = {k: v for k, v in self.tools.items() if availability.get(k, False)}
        
        logger.info(f"Available tools: {list(available_tools.keys())}")
        
        # Test with available tools
        detection_results = []
        tool_performance = {}
        
        for tool_key, tool_config in available_tools.items():
            logger.info(f"üîç Testing with {tool_config['name']}")
            
            tool_results = self._test_tool_on_dataset(tool_config, samples, dataset_name)
            detection_results.extend(tool_results)
            
            # Calculate tool performance
            tool_performance[tool_key] = self._calculate_tool_metrics(tool_results)
        
        # Calculate overall metrics
        overall_metrics = self._calculate_overall_metrics(detection_results, len(samples))
        
        return ComprehensiveBenchmark(
            dataset_name=dataset_name,
            total_samples=len(samples),
            detection_results=detection_results,
            overall_metrics=overall_metrics,
            tool_performance=tool_performance,
            timestamp=datetime.now().isoformat()
        )
    
    def _test_tool_on_dataset(self, tool_config: Dict, samples: List[Dict], 
                             dataset_name: str) -> List[DetectionResult]:
        """Test a specific tool on all samples"""
        
        results = []
        
        for i, sample in enumerate(samples):
            cve_id = sample.get('cve_id', f'sample_{i}')
            vulnerable_code = sample.get('vulnerable_code', '')
            
            if not vulnerable_code:
                logger.warning(f"No vulnerable code found for {cve_id}")
                continue
            
            logger.info(f"Testing {cve_id} with {tool_config['name']}")
            
            result = self._test_tool_on_sample(tool_config, cve_id, vulnerable_code)
            results.append(result)
        
        return results
    
    def _test_tool_on_sample(self, tool_config: Dict, cve_id: str, 
                            code: str) -> DetectionResult:
        """Test a specific tool on a single code sample"""
        
        start_time = time.time()
        detected = False
        confidence = 0.0
        tool_output = ""
        error_message = None
        
        try:
            # Create temporary C file
            with tempfile.NamedTemporaryFile(mode='w', suffix='.c', delete=False) as f:
                f.write(code)
                temp_file = f.name
            
            # Run the tool
            cmd = [tool_config['command']] + tool_config['args'] + [temp_file]
            
            result = subprocess.run(
                cmd, 
                capture_output=True, 
                text=True, 
                timeout=30
            )
            
            tool_output = result.stdout + result.stderr
            
            # Analyze output based on tool type
            detected, confidence = self._analyze_tool_output(tool_output, tool_config)
            
            # Cleanup
            os.unlink(temp_file)
            
        except subprocess.TimeoutExpired:
            error_message = "Tool execution timed out"
        except Exception as e:
            error_message = f"Tool execution failed: {str(e)}"
        
        detection_time = time.time() - start_time
        
        return DetectionResult(
            cve_id=cve_id,
            tool_name=tool_config['name'],
            detected=detected,
            confidence=confidence,
            detection_time=detection_time,
            tool_output=tool_output,
            error_message=error_message
        )
    
    def _analyze_tool_output(self, output: str, tool_config: Dict) -> Tuple[bool, float]:
        """Analyze tool output to determine detection and confidence"""
        
        tool_name = tool_config['name']
        output_lower = output.lower()
        
        # Tool-specific analysis
        if tool_name == 'Cppcheck':
            return self._analyze_cppcheck_output(output_lower)
        elif tool_name == 'Clang Static Analyzer':
            return self._analyze_clang_output(output_lower)
        elif tool_name == 'GCC Security Warnings':
            return self._analyze_gcc_output(output_lower)
        elif tool_name == 'Flawfinder':
            return self._analyze_flawfinder_output(output_lower)
        elif tool_name == 'Bandit':
            return self._analyze_bandit_output(output_lower)
        elif tool_name == 'Semgrep':
            return self._analyze_semgrep_output(output_lower)
        elif tool_name == 'Infer':
            return self._analyze_infer_output(output_lower)
        elif tool_name == 'SpotBugs':
            return self._analyze_spotbugs_output(output_lower)
        else:
            return self._analyze_generic_output(output_lower)
    
    def _analyze_cppcheck_output(self, output: str) -> Tuple[bool, float]:
        """Analyze Cppcheck output"""
        
        # Look for error/warning patterns
        error_patterns = ['error:', 'warning:', 'style:', 'performance:', 'portability:']
        error_count = sum(1 for pattern in error_patterns if pattern in output)
        
        detected = error_count > 0
        confidence = min(error_count / len(error_patterns), 1.0)
        
        return detected, confidence
    
    def _analyze_clang_output(self, output: str) -> Tuple[bool, float]:
        """Analyze Clang static analyzer output"""
        
        # Look for analyzer warnings
        warning_patterns = ['warning:', 'error:', 'note:', 'analyzer']
        warning_count = sum(1 for pattern in warning_patterns if pattern in output)
        
        detected = warning_count > 0
        confidence = min(warning_count / len(warning_patterns), 1.0)
        
        return detected, confidence
    
    def _analyze_gcc_output(self, output: str) -> Tuple[bool, float]:
        """Analyze GCC security warnings output"""
        
        # Look for security warnings
        security_patterns = ['warning:', 'error:', 'format-security', 'format-overflow']
        security_count = sum(1 for pattern in security_patterns if pattern in output)
        
        detected = security_count > 0
        confidence = min(security_count / len(security_patterns), 1.0)
        
        return detected, confidence
    
    def _analyze_flawfinder_output(self, output: str) -> Tuple[bool, float]:
        """Analyze Flawfinder output"""
        
        # Look for flawfinder findings
        flaw_patterns = ['hits =', 'risk level', 'buffer overflow', 'format string']
        flaw_count = sum(1 for pattern in flaw_patterns if pattern in output)
        
        detected = flaw_count > 0
        confidence = min(flaw_count / len(flaw_patterns), 1.0)
        
        return detected, confidence
    
    def _analyze_bandit_output(self, output: str) -> Tuple[bool, float]:
        """Analyze Bandit output"""
        
        # Look for security issues
        security_keywords = ['high', 'medium', 'low', 'issue', 'vulnerability', 'security']
        issue_count = sum(1 for keyword in security_keywords if keyword in output)
        
        detected = issue_count > 0
        confidence = min(issue_count / len(security_keywords), 1.0)
        
        return detected, confidence
    
    def _analyze_semgrep_output(self, output: str) -> Tuple[bool, float]:
        """Analyze Semgrep output"""
        
        # Look for findings
        finding_keywords = ['finding', 'rule', 'severity', 'warning', 'error']
        finding_count = sum(1 for keyword in finding_keywords if keyword in output)
        
        detected = finding_count > 0
        confidence = min(finding_count / len(finding_keywords), 1.0)
        
        return detected, confidence
    
    def _analyze_infer_output(self, output: str) -> Tuple[bool, float]:
        """Analyze Infer output"""
        
        # Look for infer findings
        infer_patterns = ['found', 'bug', 'error', 'warning', 'infer']
        infer_count = sum(1 for pattern in infer_patterns if pattern in output)
        
        detected = infer_count > 0
        confidence = min(infer_count / len(infer_patterns), 1.0)
        
        return detected, confidence
    
    def _analyze_spotbugs_output(self, output: str) -> Tuple[bool, float]:
        """Analyze SpotBugs output"""
        
        # Look for spotbugs findings
        spotbugs_patterns = ['bug', 'warning', 'error', 'spotbugs']
        spotbugs_count = sum(1 for pattern in spotbugs_patterns if pattern in output)
        
        detected = spotbugs_count > 0
        confidence = min(spotbugs_count / len(spotbugs_patterns), 1.0)
        
        return detected, confidence
    
    def _analyze_generic_output(self, output: str) -> Tuple[bool, float]:
        """Analyze generic tool output"""
        
        # Generic analysis
        generic_keywords = ['warning', 'error', 'issue', 'problem']
        keyword_count = sum(1 for keyword in generic_keywords if keyword in output)
        
        detected = keyword_count > 0
        confidence = min(keyword_count / len(generic_keywords), 1.0)
        
        return detected, confidence
    
    def _calculate_tool_metrics(self, results: List[DetectionResult]) -> Dict:
        """Calculate performance metrics for a specific tool"""
        
        if not results:
            return {
                'detection_rate': 0.0,
                'average_confidence': 0.0,
                'average_detection_time': 0.0,
                'total_detections': 0,
                'total_samples': 0,
                'error_rate': 0.0
            }
        
        total_samples = len(results)
        detections = sum(1 for r in results if r.detected)
        errors = sum(1 for r in results if r.error_message)
        
        detection_rate = detections / total_samples if total_samples > 0 else 0.0
        error_rate = errors / total_samples if total_samples > 0 else 0.0
        
        avg_confidence = sum(r.confidence for r in results) / total_samples if total_samples > 0 else 0.0
        avg_detection_time = sum(r.detection_time for r in results) / total_samples if total_samples > 0 else 0.0
        
        return {
            'detection_rate': detection_rate,
            'average_confidence': avg_confidence,
            'average_detection_time': avg_detection_time,
            'total_detections': detections,
            'total_samples': total_samples,
            'error_rate': error_rate
        }
    
    def _calculate_overall_metrics(self, results: List[DetectionResult], 
                                 total_samples: int) -> Dict:
        """Calculate overall benchmark metrics"""
        
        if not results:
            return {
                'overall_detection_rate': 0.0,
                'tools_tested': 0,
                'total_detections': 0,
                'average_confidence': 0.0,
                'average_detection_time': 0.0
            }
        
        # Group by tool
        tool_groups = {}
        for result in results:
            if result.tool_name not in tool_groups:
                tool_groups[result.tool_name] = []
            tool_groups[result.tool_name].append(result)
        
        # Calculate metrics
        total_detections = sum(1 for r in results if r.detected)
        overall_detection_rate = total_detections / len(results) if results else 0.0
        
        avg_confidence = sum(r.confidence for r in results) / len(results) if results else 0.0
        avg_detection_time = sum(r.detection_time for r in results) / len(results) if results else 0.0
        
        return {
            'overall_detection_rate': overall_detection_rate,
            'tools_tested': len(tool_groups),
            'total_detections': total_detections,
            'average_confidence': avg_confidence,
            'average_detection_time': avg_detection_time
        }
    
    def save_results(self, benchmark_result: ComprehensiveBenchmark, output_file: str):
        """Save benchmark results to file"""
        
        # Convert to serializable format
        result_dict = {
            'dataset_name': benchmark_result.dataset_name,
            'total_samples': benchmark_result.total_samples,
            'overall_metrics': benchmark_result.overall_metrics,
            'tool_performance': benchmark_result.tool_performance,
            'timestamp': benchmark_result.timestamp,
            'detection_results': [
                {
                    'cve_id': r.cve_id,
                    'tool_name': r.tool_name,
                    'detected': r.detected,
                    'confidence': r.confidence,
                    'detection_time': r.detection_time,
                    'tool_output': r.tool_output,
                    'error_message': r.error_message
                }
                for r in benchmark_result.detection_results
            ]
        }
        
        with open(output_file, 'w') as f:
            json.dump(result_dict, f, indent=2)
        
        logger.info(f"Results saved to {output_file}")
    
    def generate_report(self, benchmark_result: ComprehensiveBenchmark) -> str:
        """Generate a human-readable benchmark report"""
        
        report = f"""
# Comprehensive Vulnerability Detection Report

## Dataset: {benchmark_result.dataset_name}
- **Total Samples:** {benchmark_result.total_samples}
- **Timestamp:** {benchmark_result.timestamp}

## Overall Performance
- **Overall Detection Rate:** {benchmark_result.overall_metrics['overall_detection_rate']:.2%}
- **Tools Tested:** {benchmark_result.overall_metrics['tools_tested']}
- **Total Detections:** {benchmark_result.overall_metrics['total_detections']}
- **Average Confidence:** {benchmark_result.overall_metrics['average_confidence']:.2f}
- **Average Detection Time:** {benchmark_result.overall_metrics['average_detection_time']:.2f}s

## Tool Performance

"""
        
        for tool_name, metrics in benchmark_result.tool_performance.items():
            tool_config = self.tools.get(tool_name, {})
            report += f"""
### {tool_config.get('name', tool_name)}
- **Type:** {tool_config.get('type', 'Unknown')}
- **Description:** {tool_config.get('description', 'N/A')}
- **ML Capabilities:** {tool_config.get('ml_capabilities', 'N/A')}
- **Detection Rate:** {metrics['detection_rate']:.2%}
- **Average Confidence:** {metrics['average_confidence']:.2f}
- **Average Detection Time:** {metrics['average_detection_time']:.2f}s
- **Total Detections:** {metrics['total_detections']}/{metrics['total_samples']}
- **Error Rate:** {metrics['error_rate']:.2%}

"""
        
        return report

def main():
    """Main function to run comprehensive vulnerability testing"""
    
    print("üîç Comprehensive Vulnerability Detection Testing")
    print("=" * 60)
    
    # Initialize testing system
    tester = ComprehensiveVulnerabilityTester()
    
    # Install tools
    print("üì¶ Installing vulnerability detection tools...")
    installation_results = tester.install_tools()
    
    # Check available tools
    availability = tester.check_tool_availability()
    available_tools = [name for name, config in tester.tools.items() if availability.get(name, False)]
    
    print(f"üîß Available Tools: {len(available_tools)}")
    
    for tool_name in available_tools:
        tool_config = tester.tools[tool_name]
        print(f"  ‚úì {tool_config['name']} ({tool_config['type']}) - {tool_config['description']}")
    
    if not available_tools:
        print("‚ö†Ô∏è No tools available. Please install tools manually.")
        return
    
    # Test the critical CVE dataset
    dataset_path = 'complete_critical_cves_training_dataset.json'
    dataset_name = 'Critical CVEs Dataset'
    sample_limit = 10  # Test first 10 samples
    
    if not os.path.exists(dataset_path):
        print(f"‚ùå Dataset not found: {dataset_path}")
        return
    
    print(f"\nüöÄ Testing {dataset_name}...")
    
    # Run test
    result = tester.test_dataset(dataset_path, dataset_name, sample_limit)
    
    if result:
        # Save results
        output_file = f"comprehensive_results_{dataset_name.lower().replace(' ', '_')}.json"
        tester.save_results(result, output_file)
        
        # Generate and save report
        report = tester.generate_report(result)
        report_file = f"comprehensive_report_{dataset_name.lower().replace(' ', '_')}.md"
        with open(report_file, 'w') as f:
            f.write(report)
        
        print(f"‚úÖ Comprehensive testing completed for {dataset_name}")
        print(f"üìä Detection Rate: {result.overall_metrics['overall_detection_rate']:.2%}")
        print(f"üíæ Results saved to {output_file}")
        print(f"üìÑ Report saved to {report_file}")
        
        # Print summary
        print(f"\nüìä Tool Performance Summary:")
        for tool_name, metrics in result.tool_performance.items():
            tool_config = tester.tools.get(tool_name, {})
            print(f"  - {tool_config.get('name', tool_name)}: {metrics['detection_rate']:.2%} detection rate")
    
    print(f"\nüéâ Comprehensive vulnerability detection testing completed!")

if __name__ == "__main__":
    main()
