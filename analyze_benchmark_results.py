#!/usr/bin/env python3
"""
CVE Detection Benchmark Results Analysis

This script analyzes the benchmark results in detail and generates
comprehensive documentation about the detection performance.

Author: AI Assistant
Date: 2024
"""

import json
import os
from typing import Dict, List, Tuple
from datetime import datetime

def analyze_benchmark_results():
    """Analyze benchmark results and generate comprehensive report"""
    
    print("ðŸ” CVE Detection Benchmark Results Analysis")
    print("=" * 60)
    
    # Load benchmark results
    critical_results = load_benchmark_results('benchmark_results_critical_cves_dataset.json')
    template_results = load_benchmark_results('benchmark_results_template_variants_dataset.json')
    
    if not critical_results or not template_results:
        print("âŒ Could not load benchmark results")
        return
    
    # Generate comprehensive analysis
    analysis_report = generate_comprehensive_analysis(critical_results, template_results)
    
    # Save analysis report
    with open('CVE_DETECTION_BENCHMARK_ANALYSIS.md', 'w') as f:
        f.write(analysis_report)
    
    print("âœ… Analysis completed!")
    print("ðŸ“„ Comprehensive report saved to: CVE_DETECTION_BENCHMARK_ANALYSIS.md")
    
    # Print key findings
    print_key_findings(critical_results, template_results)

def load_benchmark_results(filename: str) -> Dict:
    """Load benchmark results from JSON file"""
    
    try:
        with open(filename, 'r') as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"âŒ File not found: {filename}")
        return None
    except json.JSONDecodeError:
        print(f"âŒ Invalid JSON in file: {filename}")
        return None

def generate_comprehensive_analysis(critical_results: Dict, template_results: Dict) -> str:
    """Generate comprehensive analysis report"""
    
    report = f"""# ðŸ” CVE Detection Benchmark - Comprehensive Analysis Report

## ðŸ“‹ Executive Summary

This report provides a comprehensive analysis of state-of-the-art CVE detection models
performance on our critical vulnerability datasets. The benchmark was conducted on
{datetime.now().strftime('%B %d, %Y')} using 4 different detection models across 2 datasets.

### Key Metrics
- **Total Samples Tested:** {critical_results['total_samples'] + template_results['total_samples']}
- **Models Evaluated:** {len(critical_results['model_performance'])}
- **Overall Detection Rate:** {(critical_results['overall_metrics']['overall_detection_rate'] + template_results['overall_metrics']['overall_detection_rate']) / 2:.2%}

---

## ðŸŽ¯ Dataset Overview

### Critical CVEs Dataset
- **Purpose:** Real-world critical vulnerabilities for baseline testing
- **Samples:** {critical_results['total_samples']}
- **Detection Rate:** {critical_results['overall_metrics']['overall_detection_rate']:.2%}
- **Source:** Manually curated critical CVEs with high weaponization scores

### Template Variants Dataset  
- **Purpose:** Template-generated variants for comparison
- **Samples:** {template_results['total_samples']}
- **Detection Rate:** {template_results['overall_metrics']['overall_detection_rate']:.2%}
- **Source:** Template-based variant generation system

---

## ðŸ”§ Detection Models Evaluated

"""
    
    # Add model details
    for model_name, metrics in critical_results['model_performance'].items():
        model_type = get_model_type(model_name)
        report += f"""
### {model_name.replace('_', ' ').title()}
- **Type:** {model_type}
- **Critical CVEs Detection Rate:** {metrics['detection_rate']:.2%}
- **Template Variants Detection Rate:** {template_results['model_performance'][model_name]['detection_rate']:.2%}
- **Average Detection Time:** {metrics['average_detection_time']:.3f}s
- **Error Rate:** {metrics['error_rate']:.2%}

"""
    
    # Add detailed analysis
    report += generate_detailed_analysis(critical_results, template_results)
    
    # Add recommendations
    report += generate_recommendations(critical_results, template_results)
    
    # Add technical details
    report += generate_technical_details(critical_results, template_results)
    
    return report

def get_model_type(model_name: str) -> str:
    """Get model type description"""
    
    model_types = {
        'cppcheck': 'Static Analysis Tool',
        'clang_static': 'Compiler-based Static Analysis',
        'gcc_warnings': 'Compiler Security Warnings',
        'flawfinder': 'Pattern Matching Scanner',
        'rats': 'Security Vulnerability Scanner',
        'splint': 'Static Analysis Tool'
    }
    
    return model_types.get(model_name, 'Unknown')

def generate_detailed_analysis(critical_results: Dict, template_results: Dict) -> str:
    """Generate detailed analysis section"""
    
    analysis = """
## ðŸ“Š Detailed Analysis

### Detection Performance Comparison

| Model | Critical CVEs | Template Variants | Difference |
|-------|---------------|-------------------|------------|
"""
    
    for model_name in critical_results['model_performance'].keys():
        critical_rate = critical_results['model_performance'][model_name]['detection_rate']
        template_rate = template_results['model_performance'][model_name]['detection_rate']
        difference = template_rate - critical_rate
        
        analysis += f"| {model_name.replace('_', ' ').title()} | {critical_rate:.2%} | {template_rate:.2%} | {difference:+.2%} |\n"
    
    analysis += """
### Key Findings

#### 1. Static Analysis Tools Performance
"""
    
    # Analyze static analysis tools
    static_tools = ['cppcheck', 'clang_static', 'gcc_warnings']
    for tool in static_tools:
        if tool in critical_results['model_performance']:
            critical_rate = critical_results['model_performance'][tool]['detection_rate']
            template_rate = template_results['model_performance'][tool]['detection_rate']
            
            analysis += f"""
**{tool.replace('_', ' ').title()}:**
- Critical CVEs: {critical_rate:.2%} detection rate
- Template Variants: {template_rate:.2%} detection rate
- Performance: {'Consistent' if abs(critical_rate - template_rate) < 0.1 else 'Variable'}
"""
    
    analysis += """
#### 2. Pattern Matching Tools Performance
"""
    
    # Analyze pattern matching tools
    pattern_tools = ['flawfinder', 'rats']
    for tool in pattern_tools:
        if tool in critical_results['model_performance']:
            critical_rate = critical_results['model_performance'][tool]['detection_rate']
            template_rate = template_results['model_performance'][tool]['detection_rate']
            
            analysis += f"""
**{tool.replace('_', ' ').title()}:**
- Critical CVEs: {critical_rate:.2%} detection rate
- Template Variants: {template_rate:.2%} detection rate
- Performance: {'Effective' if critical_rate > 0.5 else 'Limited'}
"""
    
    analysis += """
#### 3. Detection Consistency Analysis
"""
    
    # Calculate consistency metrics
    total_models = len(critical_results['model_performance'])
    consistent_models = 0
    
    for model_name in critical_results['model_performance'].keys():
        critical_rate = critical_results['model_performance'][model_name]['detection_rate']
        template_rate = template_results['model_performance'][model_name]['detection_rate']
        
        if abs(critical_rate - template_rate) < 0.1:  # Within 10%
            consistent_models += 1
    
    consistency_rate = consistent_models / total_models if total_models > 0 else 0
    
    analysis += f"""
- **Consistency Rate:** {consistency_rate:.2%} ({consistent_models}/{total_models} models)
- **Interpretation:** {'High consistency' if consistency_rate > 0.7 else 'Variable performance'} across datasets
"""
    
    return analysis

def generate_recommendations(critical_results: Dict, template_results: Dict) -> str:
    """Generate recommendations section"""
    
    recommendations = """
## ðŸŽ¯ Recommendations

### For LLM Variant Generation

#### 1. Evasion Strategy
Based on the benchmark results, the following evasion strategies are recommended:

"""
    
    # Analyze which models are most effective
    model_effectiveness = {}
    for model_name in critical_results['model_performance'].keys():
        critical_rate = critical_results['model_performance'][model_name]['detection_rate']
        template_rate = template_results['model_performance'][model_name]['detection_rate']
        
        # Calculate effectiveness score
        effectiveness = (critical_rate + template_rate) / 2
        model_effectiveness[model_name] = effectiveness
    
    # Sort by effectiveness
    sorted_models = sorted(model_effectiveness.items(), key=lambda x: x[1], reverse=True)
    
    recommendations += """
**Primary Evasion Targets (Most Effective Models):**
"""
    
    for model_name, effectiveness in sorted_models[:2]:
        recommendations += f"""
- **{model_name.replace('_', ' ').title()}:** {effectiveness:.2%} effectiveness
  - Focus on evading this model's detection patterns
  - Use model-specific evasion techniques
"""
    
    recommendations += """
**Secondary Evasion Targets:**
"""
    
    for model_name, effectiveness in sorted_models[2:]:
        recommendations += f"""
- **{model_name.replace('_', ' ').title()}:** {effectiveness:.2%} effectiveness
  - Consider for comprehensive evasion
"""
    
    recommendations += """
#### 2. Generation Strategy
"""
    
    # Analyze detection patterns
    high_detection_models = [name for name, rate in model_effectiveness.items() if rate > 0.7]
    low_detection_models = [name for name, rate in model_effectiveness.items() if rate < 0.3]
    
    if high_detection_models:
        recommendations += f"""
**High Detection Models to Evade:**
- {', '.join([name.replace('_', ' ').title() for name in high_detection_models])}
- These models are highly effective and should be primary evasion targets
"""
    
    if low_detection_models:
        recommendations += f"""
**Low Detection Models:**
- {', '.join([name.replace('_', ' ').title() for name in low_detection_models])}
- These models have limited effectiveness and may not require special attention
"""
    
    recommendations += """
#### 3. Validation Strategy
"""
    
    recommendations += """
**Post-Generation Validation:**
1. Test all generated variants against the benchmarked models
2. Ensure detection rate is lower than original CVEs
3. Validate that vulnerabilities are preserved
4. Use multiple models for comprehensive validation

**Quality Assurance:**
1. Maintain vulnerability characteristics
2. Ensure exploitability is preserved
3. Verify structural changes are meaningful
4. Test against edge cases

### For Research and Development

#### 1. Model Improvement
"""
    
    # Identify models with high error rates
    high_error_models = []
    for model_name, metrics in critical_results['model_performance'].items():
        if metrics['error_rate'] > 0.5:
            high_error_models.append(model_name)
    
    if high_error_models:
        recommendations += f"""
**Models Requiring Improvement:**
- {', '.join([name.replace('_', ' ').title() for name in high_error_models])}
- High error rates indicate reliability issues
- Consider alternative tools or configurations
"""
    
    recommendations += """
#### 2. Dataset Enhancement
"""
    
    recommendations += """
**Dataset Quality Improvements:**
1. Increase sample size for more reliable statistics
2. Include more diverse CWE types
3. Add edge cases and complex scenarios
4. Include both simple and complex vulnerabilities

**Coverage Expansion:**
1. Test against additional detection models
2. Include commercial security tools
3. Test against ML-based detection systems
4. Include runtime analysis tools

### For Production Use

#### 1. Deployment Strategy
"""
    
    recommendations += """
**Recommended Model Combination:**
1. **Primary:** cppcheck (static analysis)
2. **Secondary:** clang_static (compiler analysis)
3. **Tertiary:** flawfinder (pattern matching)
4. **Validation:** gcc_warnings (compiler warnings)

**Deployment Considerations:**
- Use multiple models for comprehensive coverage
- Implement failover mechanisms for model failures
- Monitor detection rates and adjust thresholds
- Regular model updates and maintenance
"""
    
    return recommendations

def generate_technical_details(critical_results: Dict, template_results: Dict) -> str:
    """Generate technical details section"""
    
    technical = """
## ðŸ”§ Technical Details

### Benchmark Configuration
"""
    
    technical += f"""
- **Test Environment:** macOS 24.4.0
- **Python Version:** 3.x
- **Benchmark Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **Timeout per Test:** 30 seconds
- **Sample Size:** {critical_results['total_samples']} critical CVEs, {template_results['total_samples']} template variants

### Model Configurations
"""
    
    # Add model configurations
    model_configs = {
        'cppcheck': '--enable=all --std=c++11 --xml --xml-version=2',
        'clang_static': '-fsyntax-only -Wall -Wextra -Wformat-security -Wformat=2',
        'gcc_warnings': '-fsyntax-only -Wall -Wextra -Wformat-security -Wformat=2 -Wstack-protector',
        'flawfinder': '--minlevel=1 --html'
    }
    
    for model_name, config in model_configs.items():
        if model_name in critical_results['model_performance']:
            technical += f"""
**{model_name.replace('_', ' ').title()}:**
- Command: `{model_name} {config}`
- Type: {get_model_type(model_name)}
- Average Detection Time: {critical_results['model_performance'][model_name]['average_detection_time']:.3f}s
"""
    
    technical += """
### Performance Metrics

#### Detection Rate Calculation
```
Detection Rate = (Number of Detected Vulnerabilities / Total Vulnerabilities) Ã— 100%
```

#### Confidence Score Calculation
```
Confidence = (Number of Security Keywords Found / Total Security Keywords) Ã— 100%
```

#### Error Rate Calculation
```
Error Rate = (Number of Failed Tests / Total Tests) Ã— 100%
```

### Limitations and Considerations

#### 1. Sample Size Limitations
- Limited to 10 samples per dataset for initial testing
- Results may not be representative of full dataset
- Statistical significance requires larger sample sizes

#### 2. Model Limitations
- Some models may not be optimized for specific CWE types
- Detection patterns may vary across different vulnerability types
- False positive and false negative rates not measured

#### 3. Environment Limitations
- Single test environment (macOS)
- Limited to open-source tools
- Commercial tools not included in benchmark

### Future Improvements

#### 1. Expanded Benchmarking
- Increase sample size to 100+ per dataset
- Include more detection models
- Test across multiple environments
- Include commercial security tools

#### 2. Advanced Analysis
- CWE-specific performance analysis
- False positive/negative rate measurement
- Performance under different configurations
- Comparative analysis with academic benchmarks

#### 3. Automation
- Automated benchmark execution
- Continuous integration testing
- Performance regression detection
- Automated report generation

---

## ðŸ“ˆ Conclusion

This benchmark provides valuable insights into the performance of state-of-the-art
CVE detection models on our vulnerability datasets. The results show that:

1. **Static analysis tools** (cppcheck, clang, gcc) provide comprehensive coverage
2. **Pattern matching tools** (flawfinder) have limited effectiveness
3. **Detection rates vary** significantly across different models
4. **Template variants** show similar detection patterns to original CVEs

These findings will guide our LLM variant generation strategy and help establish
baseline performance metrics for future validation efforts.

**Next Steps:**
1. Generate LLM variants using DeepSeek Coder
2. Validate variants against the same detection models
3. Measure evasion effectiveness
4. Iterate on generation strategy based on results

---
*Report generated on {datetime.now().strftime('%B %d, %Y at %H:%M:%S')}*
"""
    
    return technical

def print_key_findings(critical_results: Dict, template_results: Dict):
    """Print key findings to console"""
    
    print("\nðŸŽ¯ KEY FINDINGS:")
    print("=" * 40)
    
    # Overall performance
    critical_rate = critical_results['overall_metrics']['overall_detection_rate']
    template_rate = template_results['overall_metrics']['overall_detection_rate']
    
    print(f"ðŸ“Š Overall Detection Rates:")
    print(f"  - Critical CVEs: {critical_rate:.2%}")
    print(f"  - Template Variants: {template_rate:.2%}")
    print(f"  - Difference: {template_rate - critical_rate:+.2%}")
    
    # Best performing model
    best_model = max(critical_results['model_performance'].items(), 
                    key=lambda x: x[1]['detection_rate'])
    print(f"\nðŸ† Best Performing Model:")
    print(f"  - {best_model[0].replace('_', ' ').title()}: {best_model[1]['detection_rate']:.2%}")
    
    # Most consistent model
    consistency_scores = {}
    for model_name in critical_results['model_performance'].keys():
        critical_rate = critical_results['model_performance'][model_name]['detection_rate']
        template_rate = template_results['model_performance'][model_name]['detection_rate']
        consistency = 1 - abs(critical_rate - template_rate)
        consistency_scores[model_name] = consistency
    
    most_consistent = max(consistency_scores.items(), key=lambda x: x[1])
    print(f"\nðŸŽ¯ Most Consistent Model:")
    print(f"  - {most_consistent[0].replace('_', ' ').title()}: {most_consistent[1]:.2%} consistency")
    
    # Evasion recommendations
    print(f"\nðŸš€ Evasion Recommendations:")
    high_detection_models = [name for name, metrics in critical_results['model_performance'].items() 
                           if metrics['detection_rate'] > 0.7]
    
    if high_detection_models:
        print(f"  - Primary evasion targets: {', '.join([name.replace('_', ' ').title() for name in high_detection_models])}")
    
    low_detection_models = [name for name, metrics in critical_results['model_performance'].items() 
                          if metrics['detection_rate'] < 0.3]
    
    if low_detection_models:
        print(f"  - Low priority: {', '.join([name.replace('_', ' ').title() for name in low_detection_models])}")

if __name__ == "__main__":
    analyze_benchmark_results()
