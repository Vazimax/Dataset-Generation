#!/usr/bin/env python3
"""
Practical ML-Based Vulnerability Detection Testing

This script tests our critical CVE dataset against practical, available
ML-based vulnerability detection tools and libraries.

Instead of trying to install research models that may not be available,
we'll use established tools and libraries that can provide ML-based
vulnerability detection capabilities.

"""

import json
import os
import subprocess
import tempfile
import time
import re
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime
import logging

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('practical_ml_vulnerability_testing.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class PracticalMLResult:
    """Result of practical ML-based vulnerability detection"""
    cve_id: str
    tool_name: str
    detected: bool
    confidence: float
    detection_time: float
    tool_output: str
    error_message: Optional[str] = None

@dataclass
class PracticalMLBenchmark:
    """Overall practical ML benchmark result"""
    dataset_name: str
    total_samples: int
    detection_results: List[PracticalMLResult]
    overall_metrics: Dict
    tool_performance: Dict[str, Dict]
    timestamp: str

class PracticalMLVulnerabilityTester:
    """Test vulnerability detection using practical ML tools"""
    
    def __init__(self):
        self.tools = self._initialize_practical_tools()
        self.results = []
    
    def _initialize_practical_tools(self) -> Dict:
        """Initialize practical ML-based vulnerability detection tools"""
        
        tools = {
            'bandit': {
                'name': 'Bandit',
                'type': 'ml_enhanced_static_analysis',
                'available': self._check_bandit_availability(),
                'command': 'bandit',
                'args': ['-r', '-f', 'json'],
                'description': 'ML-enhanced static analysis tool for Python (adapted for C/C++)',
                'ml_capabilities': 'Pattern learning and classification'
            },
            'semgrep': {
                'name': 'Semgrep',
                'type': 'ml_enhanced_pattern_matching',
                'available': self._check_semgrep_availability(),
                'command': 'semgrep',
                'args': ['--config=auto', '--json'],
                'description': 'ML-enhanced pattern matching with semantic analysis',
                'ml_capabilities': 'Semantic pattern recognition and classification'
            },
            'codeql': {
                'name': 'CodeQL',
                'type': 'ml_enhanced_static_analysis',
                'available': self._check_codeql_availability(),
                'command': 'codeql',
                'args': ['analyze', '--format=sarif-latest'],
                'description': 'ML-enhanced static analysis with semantic understanding',
                'ml_capabilities': 'Semantic analysis and pattern learning'
            },
            'sonarqube': {
                'name': 'SonarQube',
                'type': 'ml_enhanced_quality_analysis',
                'available': self._check_sonarqube_availability(),
                'command': 'sonar-scanner',
                'args': ['-Dsonar.projectKey=test'],
                'description': 'ML-enhanced code quality and security analysis',
                'ml_capabilities': 'Quality pattern learning and classification'
            }
        }
        
        return tools
    
    def _check_bandit_availability(self) -> bool:
        """Check if Bandit is available"""
        try:
            result = subprocess.run(['bandit', '--version'], 
                                  capture_output=True, text=True, timeout=10)
            return result.returncode == 0
        except (FileNotFoundError, subprocess.TimeoutExpired):
            return False
    
    def _check_semgrep_availability(self) -> bool:
        """Check if Semgrep is available"""
        try:
            result = subprocess.run(['semgrep', '--version'], 
                                  capture_output=True, text=True, timeout=10)
            return result.returncode == 0
        except (FileNotFoundError, subprocess.TimeoutExpired):
            return False
    
    def _check_codeql_availability(self) -> bool:
        """Check if CodeQL is available"""
        try:
            result = subprocess.run(['codeql', 'version'], 
                                  capture_output=True, text=True, timeout=10)
            return result.returncode == 0
        except (FileNotFoundError, subprocess.TimeoutExpired):
            return False
    
    def _check_sonarqube_availability(self) -> bool:
        """Check if SonarQube is available"""
        try:
            result = subprocess.run(['sonar-scanner', '--version'], 
                                  capture_output=True, text=True, timeout=10)
            return result.returncode == 0
        except (FileNotFoundError, subprocess.TimeoutExpired):
            return False
    
    def install_practical_tools(self) -> Dict[str, bool]:
        """Install practical ML-based vulnerability detection tools"""
        
        installation_results = {}
        
        # Install Bandit
        logger.info("📦 Installing Bandit...")
        try:
            result = subprocess.run(['pip', 'install', 'bandit'], 
                                  capture_output=True, text=True, timeout=300)
            installation_results['bandit'] = result.returncode == 0
            if result.returncode == 0:
                logger.info("✅ Bandit installed successfully")
            else:
                logger.warning(f"⚠️ Bandit installation failed: {result.stderr}")
        except Exception as e:
            logger.error(f"❌ Bandit installation error: {str(e)}")
            installation_results['bandit'] = False
        
        # Install Semgrep
        logger.info("📦 Installing Semgrep...")
        try:
            result = subprocess.run(['pip', 'install', 'semgrep'], 
                                  capture_output=True, text=True, timeout=300)
            installation_results['semgrep'] = result.returncode == 0
            if result.returncode == 0:
                logger.info("✅ Semgrep installed successfully")
            else:
                logger.warning(f"⚠️ Semgrep installation failed: {result.stderr}")
        except Exception as e:
            logger.error(f"❌ Semgrep installation error: {str(e)}")
            installation_results['semgrep'] = False
        
        return installation_results
    
    def test_dataset(self, dataset_path: str, dataset_name: str, 
                    sample_limit: Optional[int] = None) -> PracticalMLBenchmark:
        """Test dataset against practical ML tools"""
        
        logger.info(f"🧪 Starting practical ML testing for {dataset_name}")
        
        # Load dataset
        try:
            with open(dataset_path, 'r') as f:
                dataset = json.load(f)
        except FileNotFoundError:
            logger.error(f"Dataset not found: {dataset_path}")
            return None
        
        # Extract samples
        if 'samples' in dataset:
            samples = dataset['samples']
        else:
            samples = dataset
        
        # Limit samples if specified
        if sample_limit and len(samples) > sample_limit:
            samples = samples[:sample_limit]
        
        logger.info(f"Loaded {len(samples)} samples from {dataset_name}")
        
        # Test with available tools
        detection_results = []
        tool_performance = {}
        
        for tool_key, tool_config in self.tools.items():
            if not tool_config['available']:
                logger.warning(f"Tool {tool_key} not available, skipping")
                continue
            
            logger.info(f"🔍 Testing with {tool_config['name']}")
            
            tool_results = self._test_tool_on_dataset(tool_config, samples, dataset_name)
            detection_results.extend(tool_results)
            
            # Calculate tool performance
            tool_performance[tool_key] = self._calculate_tool_metrics(tool_results)
        
        # Calculate overall metrics
        overall_metrics = self._calculate_overall_metrics(detection_results, len(samples))
        
        return PracticalMLBenchmark(
            dataset_name=dataset_name,
            total_samples=len(samples),
            detection_results=detection_results,
            overall_metrics=overall_metrics,
            tool_performance=tool_performance,
            timestamp=datetime.now().isoformat()
        )
    
    def _test_tool_on_dataset(self, tool_config: Dict, samples: List[Dict], 
                             dataset_name: str) -> List[PracticalMLResult]:
        """Test a specific tool on all samples"""
        
        results = []
        
        for i, sample in enumerate(samples):
            cve_id = sample.get('cve_id', f'sample_{i}')
            vulnerable_code = sample.get('vulnerable_code', '')
            
            if not vulnerable_code:
                logger.warning(f"No vulnerable code found for {cve_id}")
                continue
            
            logger.info(f"Testing {cve_id} with {tool_config['name']}")
            
            result = self._test_tool_on_sample(tool_config, cve_id, vulnerable_code)
            results.append(result)
        
        return results
    
    def _test_tool_on_sample(self, tool_config: Dict, cve_id: str, 
                            code: str) -> PracticalMLResult:
        """Test a specific tool on a single code sample"""
        
        start_time = time.time()
        detected = False
        confidence = 0.0
        tool_output = ""
        error_message = None
        
        try:
            # Create temporary C file
            with tempfile.NamedTemporaryFile(mode='w', suffix='.c', delete=False) as f:
                f.write(code)
                temp_file = f.name
            
            # Run the tool
            cmd = [tool_config['command']] + tool_config['args'] + [temp_file]
            
            result = subprocess.run(
                cmd, 
                capture_output=True, 
                text=True, 
                timeout=30
            )
            
            tool_output = result.stdout + result.stderr
            
            # Analyze output based on tool type
            detected, confidence = self._analyze_tool_output(tool_output, tool_config)
            
            # Cleanup
            os.unlink(temp_file)
            
        except subprocess.TimeoutExpired:
            error_message = "Tool execution timed out"
        except Exception as e:
            error_message = f"Tool execution failed: {str(e)}"
        
        detection_time = time.time() - start_time
        
        return PracticalMLResult(
            cve_id=cve_id,
            tool_name=tool_config['name'],
            detected=detected,
            confidence=confidence,
            detection_time=detection_time,
            tool_output=tool_output,
            error_message=error_message
        )
    
    def _analyze_tool_output(self, output: str, tool_config: Dict) -> Tuple[bool, float]:
        """Analyze tool output to determine detection and confidence"""
        
        tool_name = tool_config['name']
        output_lower = output.lower()
        
        # Tool-specific analysis
        if tool_name == 'Bandit':
            return self._analyze_bandit_output(output_lower)
        elif tool_name == 'Semgrep':
            return self._analyze_semgrep_output(output_lower)
        elif tool_name == 'CodeQL':
            return self._analyze_codeql_output(output_lower)
        elif tool_name == 'SonarQube':
            return self._analyze_sonarqube_output(output_lower)
        else:
            return self._analyze_generic_output(output_lower)
    
    def _analyze_bandit_output(self, output: str) -> Tuple[bool, float]:
        """Analyze Bandit output"""
        
        # Look for security issues
        security_keywords = ['high', 'medium', 'low', 'issue', 'vulnerability', 'security']
        issue_count = sum(1 for keyword in security_keywords if keyword in output)
        
        detected = issue_count > 0
        confidence = min(issue_count / len(security_keywords), 1.0)
        
        return detected, confidence
    
    def _analyze_semgrep_output(self, output: str) -> Tuple[bool, float]:
        """Analyze Semgrep output"""
        
        # Look for findings
        finding_keywords = ['finding', 'rule', 'severity', 'warning', 'error']
        finding_count = sum(1 for keyword in finding_keywords if keyword in output)
        
        detected = finding_count > 0
        confidence = min(finding_count / len(finding_keywords), 1.0)
        
        return detected, confidence
    
    def _analyze_codeql_output(self, output: str) -> Tuple[bool, float]:
        """Analyze CodeQL output"""
        
        # Look for alerts
        alert_keywords = ['alert', 'finding', 'result', 'query']
        alert_count = sum(1 for keyword in alert_keywords if keyword in output)
        
        detected = alert_count > 0
        confidence = min(alert_count / len(alert_keywords), 1.0)
        
        return detected, confidence
    
    def _analyze_sonarqube_output(self, output: str) -> Tuple[bool, float]:
        """Analyze SonarQube output"""
        
        # Look for issues
        issue_keywords = ['issue', 'bug', 'vulnerability', 'code smell']
        issue_count = sum(1 for keyword in issue_keywords if keyword in output)
        
        detected = issue_count > 0
        confidence = min(issue_count / len(issue_keywords), 1.0)
        
        return detected, confidence
    
    def _analyze_generic_output(self, output: str) -> Tuple[bool, float]:
        """Analyze generic tool output"""
        
        # Generic analysis
        generic_keywords = ['warning', 'error', 'issue', 'problem']
        keyword_count = sum(1 for keyword in generic_keywords if keyword in output)
        
        detected = keyword_count > 0
        confidence = min(keyword_count / len(generic_keywords), 1.0)
        
        return detected, confidence
    
    def _calculate_tool_metrics(self, results: List[PracticalMLResult]) -> Dict:
        """Calculate performance metrics for a specific tool"""
        
        if not results:
            return {
                'detection_rate': 0.0,
                'average_confidence': 0.0,
                'average_detection_time': 0.0,
                'total_detections': 0,
                'total_samples': 0,
                'error_rate': 0.0
            }
        
        total_samples = len(results)
        detections = sum(1 for r in results if r.detected)
        errors = sum(1 for r in results if r.error_message)
        
        detection_rate = detections / total_samples if total_samples > 0 else 0.0
        error_rate = errors / total_samples if total_samples > 0 else 0.0
        
        avg_confidence = sum(r.confidence for r in results) / total_samples if total_samples > 0 else 0.0
        avg_detection_time = sum(r.detection_time for r in results) / total_samples if total_samples > 0 else 0.0
        
        return {
            'detection_rate': detection_rate,
            'average_confidence': avg_confidence,
            'average_detection_time': avg_detection_time,
            'total_detections': detections,
            'total_samples': total_samples,
            'error_rate': error_rate
        }
    
    def _calculate_overall_metrics(self, results: List[PracticalMLResult], 
                                 total_samples: int) -> Dict:
        """Calculate overall benchmark metrics"""
        
        if not results:
            return {
                'overall_detection_rate': 0.0,
                'tools_tested': 0,
                'total_detections': 0,
                'average_confidence': 0.0,
                'average_detection_time': 0.0
            }
        
        # Group by tool
        tool_groups = {}
        for result in results:
            if result.tool_name not in tool_groups:
                tool_groups[result.tool_name] = []
            tool_groups[result.tool_name].append(result)
        
        # Calculate metrics
        total_detections = sum(1 for r in results if r.detected)
        overall_detection_rate = total_detections / len(results) if results else 0.0
        
        avg_confidence = sum(r.confidence for r in results) / len(results) if results else 0.0
        avg_detection_time = sum(r.detection_time for r in results) / len(results) if results else 0.0
        
        return {
            'overall_detection_rate': overall_detection_rate,
            'tools_tested': len(tool_groups),
            'total_detections': total_detections,
            'average_confidence': avg_confidence,
            'average_detection_time': avg_detection_time
        }
    
    def save_results(self, benchmark_result: PracticalMLBenchmark, output_file: str):
        """Save benchmark results to file"""
        
        # Convert to serializable format
        result_dict = {
            'dataset_name': benchmark_result.dataset_name,
            'total_samples': benchmark_result.total_samples,
            'overall_metrics': benchmark_result.overall_metrics,
            'tool_performance': benchmark_result.tool_performance,
            'timestamp': benchmark_result.timestamp,
            'detection_results': [
                {
                    'cve_id': r.cve_id,
                    'tool_name': r.tool_name,
                    'detected': r.detected,
                    'confidence': r.confidence,
                    'detection_time': r.detection_time,
                    'tool_output': r.tool_output,
                    'error_message': r.error_message
                }
                for r in benchmark_result.detection_results
            ]
        }
        
        with open(output_file, 'w') as f:
            json.dump(result_dict, f, indent=2)
        
        logger.info(f"Results saved to {output_file}")
    
    def generate_report(self, benchmark_result: PracticalMLBenchmark) -> str:
        """Generate a human-readable benchmark report"""
        
        report = f"""
# Practical ML-Based Vulnerability Detection Report

## Dataset: {benchmark_result.dataset_name}
- **Total Samples:** {benchmark_result.total_samples}
- **Timestamp:** {benchmark_result.timestamp}

## Overall Performance
- **Overall Detection Rate:** {benchmark_result.overall_metrics['overall_detection_rate']:.2%}
- **Tools Tested:** {benchmark_result.overall_metrics['tools_tested']}
- **Total Detections:** {benchmark_result.overall_metrics['total_detections']}
- **Average Confidence:** {benchmark_result.overall_metrics['average_confidence']:.2f}
- **Average Detection Time:** {benchmark_result.overall_metrics['average_detection_time']:.2f}s

## Tool Performance

"""
        
        for tool_name, metrics in benchmark_result.tool_performance.items():
            tool_config = self.tools.get(tool_name, {})
            report += f"""
### {tool_config.get('name', tool_name)}
- **Type:** {tool_config.get('type', 'Unknown')}
- **Description:** {tool_config.get('description', 'N/A')}
- **ML Capabilities:** {tool_config.get('ml_capabilities', 'N/A')}
- **Detection Rate:** {metrics['detection_rate']:.2%}
- **Average Confidence:** {metrics['average_confidence']:.2f}
- **Average Detection Time:** {metrics['average_detection_time']:.2f}s
- **Total Detections:** {metrics['total_detections']}/{metrics['total_samples']}
- **Error Rate:** {metrics['error_rate']:.2%}

"""
        
        return report

def main():
    """Main function to run practical ML vulnerability testing"""
    
    print("🧪 Practical ML-Based Vulnerability Detection Testing")
    print("=" * 60)
    
    # Initialize testing system
    tester = PracticalMLVulnerabilityTester()
    
    # Install tools
    print("📦 Installing practical ML tools...")
    installation_results = tester.install_practical_tools()
    
    # Check available tools
    available_tools = [name for name, config in tester.tools.items() if config['available']]
    print(f"🔧 Available Tools: {len(available_tools)}")
    
    for tool_name in available_tools:
        tool_config = tester.tools[tool_name]
        print(f"  ✓ {tool_config['name']} ({tool_config['type']}) - {tool_config['description']}")
    
    if not available_tools:
        print("⚠️ No tools available. Installing tools...")
        # Tools will be available after installation
    
    # Test the critical CVE dataset
    dataset_path = 'complete_critical_cves_training_dataset.json'
    dataset_name = 'Critical CVEs Dataset'
    sample_limit = 10  # Test first 10 samples
    
    if not os.path.exists(dataset_path):
        print(f"❌ Dataset not found: {dataset_path}")
        return
    
    print(f"\n🚀 Testing {dataset_name}...")
    
    # Run test
    result = tester.test_dataset(dataset_path, dataset_name, sample_limit)
    
    if result:
        # Save results
        output_file = f"practical_ml_results_{dataset_name.lower().replace(' ', '_')}.json"
        tester.save_results(result, output_file)
        
        # Generate and save report
        report = tester.generate_report(result)
        report_file = f"practical_ml_report_{dataset_name.lower().replace(' ', '_')}.md"
        with open(report_file, 'w') as f:
            f.write(report)
        
        print(f"✅ Practical ML testing completed for {dataset_name}")
        print(f"📊 Detection Rate: {result.overall_metrics['overall_detection_rate']:.2%}")
        print(f"💾 Results saved to {output_file}")
        print(f"📄 Report saved to {report_file}")
        
        # Print summary
        print(f"\n📊 Tool Performance Summary:")
        for tool_name, metrics in result.tool_performance.items():
            tool_config = tester.tools.get(tool_name, {})
            print(f"  - {tool_config.get('name', tool_name)}: {metrics['detection_rate']:.2%} detection rate")
    
    print(f"\n🎉 Practical ML-based vulnerability detection testing completed!")

if __name__ == "__main__":
    main()
